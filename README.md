# Chatbot Healthcare v2

Há»‡ thá»‘ng chatbot chÄƒm sÃ³c sá»©c khá»e sá»­ dá»¥ng RAG (Retrieval-Augmented Generation) vá»›i Ollama vÃ  HuggingFace models. Backend Ä‘Æ°á»£c xÃ¢y dá»±ng vá»›i **FastAPI** Ä‘á»ƒ cÃ³ hiá»‡u suáº¥t cao vÃ  API documentation tá»± Ä‘á»™ng.

## ğŸš€ Quick Start

### YÃªu cáº§u
- Docker & Docker Compose
- NVIDIA Docker runtime (Ä‘á»ƒ sá»­ dá»¥ng GPU)
- Ãt nháº¥t 16GB RAM
- 50GB disk space (cho models)

### Cháº¡y vá»›i Docker Compose

```bash
# Clone repository
git clone <your-repo>
cd chatbot_healthcare_v2

# Build vÃ  start services vá»›i FastAPI
docker-compose up -d

# Hoáº·c sá»­ dá»¥ng Makefile
make build
make up
```

### Truy cáº­p á»©ng dá»¥ng

- **Web UI**: http://localhost:80
- **Ollama API**: http://localhost:11434
- **Health check**: http://localhost:80/ping

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
chatbot_healthcare_v2/
â”œâ”€â”€ app.py                     # Flask application
â”œâ”€â”€ rag_inference.py           # RAG logic vÃ  model
â”œâ”€â”€ Dockerfile                 # Container definition
â”œâ”€â”€ docker-compose.yml        # Service orchestration
â”œâ”€â”€ docker-compose.override.yml # Development overrides
â”œâ”€â”€ .env                       # Environment variables
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ entrypoint.sh             # Container startup script
â”œâ”€â”€ download_models.py        # Model download utility
â”œâ”€â”€ Makefile                  # Helper commands
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html            # Web interface
â”œâ”€â”€ static/                   # Static assets
â”œâ”€â”€ faiss_index_document_GDM/ # Vector database
â””â”€â”€ temp/                     # Temporary files
```

## ğŸ› ï¸ Commands

### Sá»­ dá»¥ng Makefile (Recommended)
```bash
make build     # Build container
make up        # Start services
make down      # Stop services
make logs      # View logs
make shell     # Access container
make clean     # Clean up
make health    # Check status
```

### Sá»­ dá»¥ng Docker Compose trá»±c tiáº¿p
```bash
# Development mode (vá»›i hot reload)
docker-compose up -d

# Production mode (khÃ´ng cÃ³ override)
docker-compose -f docker-compose.yml up -d

# Xem logs
docker-compose logs -f

# Stop services
docker-compose down

# Clean up (cáº©n tháº­n - sáº½ xÃ³a volumes)
docker-compose down -v
```

## âš™ï¸ Configuration

### Environment Variables

Chá»‰nh sá»­a file `.env`:

```bash
# Ports
APP_PORT=80
OLLAMA_PORT=11434

# Models
OLLAMA_LLM_MODEL=qwen2.5:14b
OLLAMA_EMBEDDING_MODEL=bge-m3
RERANKER_MODEL=BAAI/bge-reranker-v2-m3

# GPU (uncomment Ä‘á»ƒ sá»­ dá»¥ng GPU cá»¥ thá»ƒ)
# CUDA_VISIBLE_DEVICES=0
```

### Custom Models

Äá»ƒ sá»­ dá»¥ng models khÃ¡c:

1. Cáº­p nháº­t `.env` file
2. Rebuild container: `make build`
3. Start láº¡i: `make up`

## ğŸ› Troubleshooting

### Container khÃ´ng start
```bash
# Check logs
make logs

# Check status
make health

# Rebuild
make clean
make build
make up
```

### Models khÃ´ng load
```bash
# Access container
make shell

# Check Ollama models
ollama list

# Check HuggingFace cache
ls -la /chatbot_healthcare/models/
```

### Out of memory
- TÄƒng Docker memory limit
- Sá»­ dá»¥ng smaller models
- Enable swap

### GPU khÃ´ng hoáº¡t Ä‘á»™ng
```bash
# Check NVIDIA runtime
docker run --rm --gpus all nvidia/cuda:11.8-base nvidia-smi

# Check trong container
make shell
nvidia-smi
```

## ğŸ“Š Monitoring

### Health Checks
```bash
# Application health
curl http://localhost:80/ping

# Ollama health
curl http://localhost:11434/api/tags

# Container status
docker-compose ps
```

### Logs
```bash
# All services
make logs

# Specific service
docker-compose logs -f chatbot-healthcare
```

## ğŸ”§ Development

### Local Development
```bash
# Start in dev mode (with file mounting)
make dev-up

# Modify files locally - changes will be reflected in container
# Hot reload enabled for Flask app
```

### Adding New Models
1. Update `download_models.py`
2. Modify `rag_inference.py`
3. Rebuild: `make build`

## ğŸ“ API Endpoints

- `GET /` - Web interface
- `POST /ask` - Chat endpoint
- `POST /upload` - Audio upload
- `GET /ping` - Health check

## ğŸš€ FastAPI Features

### API Documentation
FastAPI tá»± Ä‘á»™ng táº¡o API documentation táº¡i:
- **Swagger UI**: `http://localhost:80/docs`
- **ReDoc**: `http://localhost:80/redoc`

### Development Mode
```bash
# Cháº¡y trong development mode vá»›i auto-reload
uvicorn app:app --host 0.0.0.0 --port 80 --reload

# Hoáº·c cháº¡y trá»±c tiáº¿p
python app.py
```

### Testing
```bash
# Test API endpoints
python test_app.py

# Hoáº·c sá»­ dá»¥ng curl
curl -X GET http://localhost:80/ping
curl -X POST http://localhost:80/ask \
  -F "question=Hello" \
  -F "role=doctor" \
  -F "responseWithAudio=false"
```

### Performance Benefits
- **Async/Await**: Há»— trá»£ báº¥t Ä‘á»“ng bá»™ native
- **High Performance**: Nhanh hÆ¡n Flask Ä‘Ã¡ng ká»ƒ
- **Type Safety**: Validation tá»± Ä‘á»™ng vá»›i Pydantic
- **Auto Documentation**: OpenAPI/Swagger tá»± Ä‘á»™ng

## ğŸ¤ Contributing

1. Fork repository
2. Make changes
3. Test with `make dev-up`
4. Submit PR

## ğŸ“„ License

[Your license here]